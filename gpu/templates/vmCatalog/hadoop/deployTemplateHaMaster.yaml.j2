heat_template_version: '2018-08-31'

description: hadoop Mulity Master template

parameter_groups:
- parameters:
  - image
  - master_flavor
  - master_flavor_volume_size
  - worker_flavor
  - worker_flavor_volume_size
  - master_cnt
  - worker_cnt
  - availability_zone
  - key_name
  - provider_net
  - root_password
  - security_group
  - api_end_point
  - provider_subnet
  - hive_password
  - private_key
  - spark_use
  - hbase_use
  - s3_endpoint
  - s3_accessKey
  - s3_secretKey
  - s3_bucket_name

parameters:
  image:
    label: "이미지"
    type: string
  availability_zone:
    label: "가용구역"
    type: string
  master_flavor_volume_size:
    type: string
  master_flavor:
    type: string
  master_cnt:
    type: string
  worker_cnt:
    type: string
  worker_flavor_volume_size:
    type: string
  worker_flavor:
    type: string
  key_name:
    label: "보안키"
    type: string
  provider_net:
    label: "provider_net"
    type: string
  api_end_point:
    label: "api_end_point"
    type: string
  provider_subnet:
    label: "provider_subnet"
    type: string
  security_group:
    label: "security_group"
    type: string
    default: default
  root_password:
    label: "root 비밀번호"
    type: string
  hive_password:
    label: "hive_password"
    type: string
  private_key:
    label: "private_key"
    type: string
  spark_use:
    label: "spark_use"
    type: string
  hbase_use:
    label: "hbase_use"
    type: string
  s3_endpoint:
    type: string
  s3_accessKey:
    type: string
  s3_secretKey:
    type: string
  s3_bucket_name:
    type: string
{% if octaviaLbUse %}
  service_port:
    type: number
    default: 50070
  lb_description:
    type: string
    default: "vmCatalog Tomcat cluster lb"
  lb_svc_port:
    type: string
  lb_svc_protocol:
    type: string
    default: TCP
  lb_algorithm:
    type: string
    default: ROUND_ROBIN
  lb_svc_connection_limit:
    type: string
    default: 2000
  lb_svc_monitor_type:
    type: string
    default: TCP
  lb_svc_monitor_delay:
    type: string
    default: 3
  lb_svc_monitor_max_retries:
    type: string
    default: 5
  lb_svc_monitor_timeout:
    type: string
    default: 5
{% if monitorUrlPathUse %}
  lb_svc_monitor_url_path:
    type: string
    default: /
{% endif %}
{% endif %}

resources:

  server1_port:
    type: OS::Neutron::Port
    properties:
      fixed_ips: [{ subnet_id: { get_param: provider_subnet } }]
      network: { get_param: provider_net }
      security_groups: [{ get_param: security_group }]

  {% set iworkerCnt = workerCnt|int %}  {# string으로 받아서 추가 #}
  {% set pworker =  iworkerCnt + 1 %}
  {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
  
  server{{server}}_port:
    type: OS::Neutron::Port
    properties:
      fixed_ips: [{ subnet_id: { get_param: provider_subnet } }]
      network: { get_param: provider_net }
      security_groups: [{ get_param: security_group }]

  {%- endfor %}
  
  init_config1:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config:
        str_replace:
          params:
            $api_end_point: { get_param: api_end_point }
            $project_id: { get_param: 'OS::project_id' }
            $stack_id: { get_param: 'OS::stack_id' }
            $stack_name: { get_param: 'OS::stack_name' }
            $private_key: { get_param: private_key }
            $s3_endpoint: { get_param: s3_endpoint }
            $s3_secretKey: { get_param: s3_secretKey }
            $s3_accessKey: { get_param: s3_accessKey }
            $root_password: { get_param: root_password }
            $hive_password: { get_param: hive_password }
            {% if octaviaLbUse %}
            $vip_address: { get_attr: [server1_port, fixed_ips, 0, ip_address] }
            $service_port: { get_param: service_port }
            {% elif %}
            $master_address: { get_attr: [server1_port, fixed_ips, 0, ip_address] }
            {% endif %}
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            $node{{worker}}_address: { get_attr: [server{{server}}_port, fixed_ips, 0, ip_address] }
            {%- endfor %}
            $server_addresses:
              list_join:
                - ','
                - - { get_attr: [server1_port, fixed_ips, 0, ip_address] }
                  {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
                  - { get_attr: [server{{server}}_port, fixed_ips, 0, ip_address] }
                  {%- endfor %}
          template: |
            #!/bin/sh
            echo "01.hadoop HAMaster install start ......"

            curl -X PUT -H "Content-Type: application/json" --connect-timeout 10 --data "{\"deployStatus\": \"DEPLOY_IN_PROGRESS\"}" $api_end_point/vm_catalog/external/$project_id/deploy/$stack_name/$stack_id/status

            cat <<EOF>> /etc/ssh/ssh_config
            Port 20022
            EOF

            sudo service sshd restart


            cat <<EOF> /home/centos/.ssh/kepri-msa_default.pem
            $private_key
            EOF

            chown centos.centos /home/centos/.ssh/kepri-msa_default.pem
            chmod 600 /home/centos/.ssh/kepri-msa_default.pem

            echo "address1=$vip_address"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            echo "address{{server}}=$node{{worker}}_address"
            {%- endfor %}

            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            masterRun=1
            while [[ masterRun -eq 1 ]]
            do
              echo "nc -z $node{{worker}}_address 20022"
              nc -z $node{{worker}}_address 20022
              masterRun=$?
              sleep 5
            done
            echo "node{{worker}} active"
            {%- endfor %}

            echo "##### 01.hadoop core setting start ###########"
            echo "[01] host setting"
            echo "##### 01.1 host file setting ###########"
            cat <<EOF>> /etc/hosts
            {% if octaviaLbUse %}
            $vip_address hmaster
            {% elif %}
            $master_address hmaster
            {% endif %}
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            $node{{worker}}_address hworker-{{worker}}
            {%- endfor %}
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            mkdir -p /home/centos/hadoop/pids
            mkdir -p /home/centos/hadoop/etc/hadoop/dfs/data
            mkdir -p /home/centos/hadoop/etc/hadoop/dfs/name
            mkdir -p /home/centos/hadoop/etc/hadoop/dfs/journalnode
            sudo chmod 755 /home/centos/hadoop/pids
            sudo chmod 755 /home/centos/hadoop/etc/hadoop/dfs/data
            sudo chmod 755 /home/centos/hadoop/etc/hadoop/dfs/name
            sudo chmod 755 /home/centos/hadoop/etc/hadoop/dfs/journalnode
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            echo "##### 01.2 add_known_hosts ###########"
            ssh-keyscan -p 20022 localhost,0.0.0.0 >> /home/centos/.ssh/known_hosts
            ssh-keyscan -p 20022 hmaster,$master_addres >> /home/centos/.ssh/known_hosts
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            ssh-keyscan -p 20022 hworker-{{worker}},$node{{worker}}_address >> /home/centos/.ssh/known_hosts
            {%- endfor %}
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            echo "##### 01.3 add_authorized_keys ###########"
            ssh-keygen -b 2048 -t rsa -f /home/centos/.ssh/id_rsa -q -N ''
            cat /home/centos/.ssh/id_rsa.pub | ssh -i "/home/centos/.ssh/kepri-msa_default.pem" centos@localhost  "cat - >> /home/centos/.ssh/authorized_keys"
            cat /home/centos/.ssh/id_rsa.pub | ssh -i "/home/centos/.ssh/kepri-msa_default.pem" centos@hmaster  "cat - >> /home/centos/.ssh/authorized_keys"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            cat /home/centos/.ssh/id_rsa.pub | ssh -i "/home/centos/.ssh/kepri-msa_default.pem" centos@hworker-{{worker}}  "cat - >> /home/centos/.ssh/authorized_keys"
            {%- endfor %}
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            cp ~/hadoop/share/hadoop/tools/lib/hadoop-aws-2.7.7.jar ~/hadoop/share/hadoop/common/lib/
            cp ~/hadoop/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar ~/hadoop/share/hadoop/common/lib/
            cp ~/hadoop/share/hadoop/tools/lib/joda-time-2.10.jar ~/hadoop/share/hadoop/common/lib/
            cp ~/hadoop/share/hadoop/tools/lib/jackson-*.jar  ~/hadoop/share/hadoop/common/lib/
            export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*
            EOF

            echo "[01]hadoop config file update"

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/hadoop_env.sh
            export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*
            EOF


            cat <<EOF>> /home/centos/hadoop/etc/hadoop/core-site.xml
            <configuration>
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://hmaster:9000</value>
            </property>
            <property>
                 <name>ha.zookeeper.quorum</name>
                 <value>hmaster:2181{%- for worker in range(1,pworker) %}{% set server = worker + 1 %},hworker-{{worker}}:2181{%- endfor %}</value>
            </property>
            <property>
                <name>fs.s3a.impl</name>
                <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
            </property>
            <property>
              <name>fs.s3a.aws.credentials.provider</name>
              <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
            </property>
            <property>
              <name>fs.s3a.access.key</name>
              <value><![CDATA[$s3_accessKey]]></value>
            </property>
            <property>
              <name>fs.s3a.secret.key</name>
              <value><![CDATA[$s3_secretKey]]></value>
            </property>
            <property>
              <name>fs.s3a.connection.ssl.enabled</name>
              <value>false</value>
            </property>
            <property>
              <name>fs.s3a.endpoint</name>
              <value><![CDATA[$s3_endpoint]]></value>
            </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/hdfs-site.xml
            <configuration>
                    <property>
                            <name>dfs.namenode.name.dir</name>
                            <value>/home/centos/hadoop/etc/hadoop/dfs/name</value>
                    </property>
                    <property>
                            <name>dfs.datanode.data.dir</name>
                            <value>/home/centos/hadoop/etc/hadoop/dfs/data</value>
                    </property>
                    <property>
                            <name>dfs.journalnode.edits.dir</name>
                            <value>/home/centos/hadoop/etc/hadoop/dfs/journal</value>
                    </property>
                    <property>
                            <name>dfs.nameservices</name>
                            <value>hadoop-cluster</value>
                    </property>
                    <property>
                            <name>dfs.ha.namenodes.hadoop-cluster</name>
                            <value>nn1,nn2</value>
                    </property>
                    <property>
                            <name>dfs.namenode.rpc-address.hadoop-cluster.nn1</name>
                            <value>hmaster:9000</value>
                    </property>
                    <property>
                            <name>dfs.namenode.rpc-address.hadoop-cluster.nn2</name>
                            <value>hworker-1:9000</value>
                    </property>
                    <property>
                            <name>dfs.namenode.http-address.hadoop-cluster.nn1</name>
                            <value>hmaster:50070</value>
                    </property>
                    <property>
                            <name>dfs.namenode.http-address.hadoop-cluster.nn2</name>
                            <value>hworker-1:50070</value>
                    </property>
                    <property>
                            <name>dfs.namenode.shared.edits.dir</name>
                            <value>qjournal://hmaster:8485{%- for worker in range(1,pworker) %}{% set server = worker + 1 %};hworker-{{worker}}:8485{%- endfor %}/hadoop-cluster</value>
                    </property>
                    <property>
                            <name>dfs.client.failover.proxy.provider.hadoop-cluster</name>
                            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
                    </property>
                    <property>
                            <name>dfs.ha.fencing.methods</name>
                            <value>sshfence</value>
                    </property>
                    <property>
                            <name>dfs.ha.fencing.ssh.private-key-files</name>
                            <value>/home/centos/.ssh/id_rsa</value>
                    </property>
                    <property>
                            <name>dfs.ha.automatic-failover.enabled</name>
                            <value>true</value>
                    </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/mapred-site.xml
            <configuration>
                <property>
                    <name>mapreduce.job.ubertask.enable</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/yarn-site.xml
            <configuration>
            <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
            </property>
            <property>
                <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>
            <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hmaster</value>
            </property>
            <property>
                <name>yarn.application.classpath</name>
                <value>
                    /home/centos/hadoop/etc/hadoop,
                    /home/centos/hadoop/share/hadoop/common/*,
                    /home/centos/hadoop/share/hadoop/common/lib/*,
                    /home/centos/hadoop/share/hadoop/hdfs/*,
                    /home/centos/hadoop/share/hadoop/hdfs/lib/*,
                    /home/centos/hadoop/share/hadoop/mapreduce/*,
                    /home/centos/hadoop/share/hadoop/mapreduce/lib/*,
                    /home/centos/hadoop/share/hadoop/yarn/*,
                    /home/centos/hadoop/share/hadoop/yarn/lib/*
                </value>
            </property>
            </configuration>
            EOF

            echo "----------config file set end"

            sudo -- sh -c "echo hworker-1 > /home/centos/hadoop/etc/hadoop/slaves"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo -- sh -c "echo hworker-{{worker}} >> /home/centos/hadoop/etc/hadoop/slaves"
            {%- endfor %}

            echo "##### 03.zookeeper setting start ###########"
            sudo -i -u centos bash << EOF
            source ~/.bashrc
            mkdir /home/centos/zookeeper/tmp
            cp /home/centos/zookeeper/conf/zoo_sample.cfg /home/centos/zookeeper/conf/zoo.cfg
            sudo -- sh -c "echo dataDir=/home/centos/zookeeper/tmp >> /home/centos/zookeeper/conf/zoo.cfg"
            sudo -- sh -c "echo server.1=hmaster:2888:3888 >> /home/centos/zookeeper/conf/zoo.cfg"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo -- sh -c "echo server.{{server}}=hworker-{{worker}}:2888:3888 >> /home/centos/zookeeper/conf/zoo.cfg"
            {%- endfor %}
            echo "1" > /home/centos/zookeeper/tmp/myid
            chown centos:centos /home/centos/zookeeper/tmp/myid
            chmod 777 /home/centos/zookeeper/tmp/myid
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            /home/centos/zookeeper/bin/zkServer.sh start
            echo "##### 03.zookeeper setting end ###########"
            EOF

            echo "#####[00]ZooKeeper all node check #####"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            masterRun=1
            while [[ masterRun -eq 1 ]]
            do
              echo "nc -z $node{{worker}}_address 3888"
              nc -z $node{{worker}}_address 3888
              masterRun=$?
              sleep 5
            done
            echo "zookeeper node{{worker}} active"
            {%- endfor %}
            echo "#####[00]ZooKeeper all node check end #####"

            echo "#####[01]ZooKeeper 초기화 #####"
            sudo su - centos -c "hdfs zkfc -formatZK"

            echo "#####[02]all server  journalnode 시작 #####"
            sudo su - centos -c "ssh hmaster '~/hadoop/sbin/hadoop-daemon.sh start journalnode'"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo su - centos -c "ssh hworker-{{worker}} '~/hadoop/sbin/hadoop-daemon.sh start journalnode'"
            {%- endfor %}

            echo "#####[03]Namenode init and  namenode run  #####"
            sudo su - centos -c "hadoop namenode -format -force "
            sudo su - centos -c "~/hadoop/sbin/hadoop-daemon.sh start namenode "

            echo "#####[04]FailOverController run  #####"
            sudo su - centos -c "~/hadoop/sbin/hadoop-daemon.sh start zkfc"
            echo "#####[05]Datanode 서버에서 Datanode 시작 #####"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo su - centos -c "ssh hworker-{{worker}} '~/hadoop/sbin/hadoop-daemon.sh start datanode'"
            {%- endfor %}

            echo "#####[06]Standby namenode 서버 실행 #####"
            sudo su - centos -c "ssh hworker-1 'hdfs namenode -bootstrapStandby'"
            sudo su - centos -c "ssh hworker-1 '~/hadoop/sbin/hadoop-daemon.sh start namenode'"

            echo "#####[07]Standby namenode FailOverController 실행 #####"
            sudo su - centos -c "ssh hworker-1 '~/hadoop/sbin/hadoop-daemon.sh start zkfc'"

            echo "#####[08]yarn 실행 #####"
            sudo su - centos -c "start-yarn.sh"

            echo "##### [05]hive setting start ###########"
            echo "mariadb setting start"
            #cp /etc/my.cnf /etc/my.cnf.backup
            #sed -i 's/^port            = 3306/port            = $service_port/' /etc/my.cnf
            sleep 1
            systemctl start mariadb
            /usr/bin/mysqladmin -u root password '$root_password'

            mysql -u root -p$root_password <<EOF
            create database hive CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
            GRANT ALL PRIVILEGES ON hive.* To 'hive'@LOCALHOST IDENTIFIED BY '$hive_password';
            EOF

            sudo systemctl enable mariadb
            echo "mariadb setting end"

            cat <<EOF> /home/centos/hive/conf/hive-site.xml
            <?xml version="1.0" encoding="UTF-8" standalone="no"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>hive.metastore.local</name>
                    <value>false</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionURL</name>
                    <value>jdbc:mariadb://localhost:3306/hive?createDatabaseIfNotExist=true</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionDriverName</name>
                    <value>org.mariadb.jdbc.Driver</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionUserName</name>
                    <value>hive</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionPassword</name>
                    <value><![CDATA[$hive_password]]></value>
                </property>
            </configuration>
            EOF

            chown centos:centos /home/centos/hive/conf/hive-site.xml
            echo "## MariaDB connetc init "
            sudo su - centos -c "/home/centos/hive/bin/schematool -initSchema -dbType mysql "
            echo "##### [05]hive setting end ###########"

{% if hbaseUse %}
            echo "##### 04.hbase setting start ###########"
            VAR1=$(cat <<EOF
            <?xml version="1.0"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>hbase.rootdir</name>
                    <value>hdfs://hmaster:9000/hbase</value>
                </property>
                <property>
                    <name>hbase.cluster.distributed</name>
                    <value>true</value>
                </property>
                <property>
                    <name>hbase.zookeeper.quorum</name>
                    <value>hmaster{%- for worker in range(1,pworker) %}{% set server = worker + 1 %},hworker-{{worker}}{%- endfor %}</value>
                </property>
                <property>
                    <name>dfs.replication</name>
                    <value>3</value>
                </property>
                <property>
                  <name>hbase.unsafe.stream.capability.enforce</name>
                  <value>false</value>
                </property>
                <property>
                    <name>hbase.zookeeper.property.clientPort</name>
                    <value>2181</value>
                </property>
            </configuration>
            EOF
            )

            cat <<EOF> /home/centos/hbase/conf/hbase-site.xml
            ${VAR1}
            EOF

            chown centos:centos /home/centos/hbase/conf/hbase-site.xml

            cat <<EOF> /home/centos/hbase/conf/regionservers
            hmaster
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            hworker-{{worker}}
            {%- endfor %}
            EOF

            chown centos:centos /home/centos/hbase/conf/regionservers

            cat <<EOF>> /home/centos/hbase/conf/hbase-env.sh
            export HBASE_MANAGES_ZK=false
            export HBASE_PID_DIR=/home/centos/hbase/pid
            EOF


            sudo -i -u centos bash << EOF
            source ~/.bashrc
            hadoop dfsadmin -safemode leave
            /home/centos/hbase/bin/start-hbase.sh
            EOF

            echo "##### 04.hbase setting end ###########"
{% endif %}
{% if sparkUse %}
            echo "##### 02.spark setting start ###########"
            sudo -i -u centos bash << EOF
            source ~/.bashrc
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo -- sh -c "echo hworker-{{worker}} >> /home/centos/spark/conf/slaves"
            {%- endfor %}
            /home/centos/spark/sbin/start-master.sh
            EOF

            echo "##### 02.spark setting end ###########"
{% endif %}
            curl -X PUT -H "Content-Type: application/json" --connect-timeout 10 --data "{\"deployStatus\": \"DEPLOY_COMPLETE\"}" $api_end_point/vm_catalog/external/$project_id/deploy/$stack_name/$stack_id/status

  {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
  init_config{{server}}:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config:
        str_replace:
          params:
            $api_end_point: { get_param: api_end_point }
            $project_id: { get_param: 'OS::project_id' }
            $stack_id: { get_param: 'OS::stack_id' }
            $stack_name: { get_param: 'OS::stack_name' }
            $private_key: { get_param: private_key }
            $s3_endpoint: { get_param: s3_endpoint }
            $s3_secretKey: { get_param: s3_secretKey }
            $s3_accessKey: { get_param: s3_accessKey }
            $root_password: { get_param: root_password }
            {% if octaviaLbUse %}
            $vip_address: { get_attr: [server1_port, fixed_ips, 0, ip_address] }
            {% elif %}
            $master_address: { get_attr: [server1_port, fixed_ips, 0, ip_address] }
            {% endif %}
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            $node{{worker}}_address: { get_attr: [server{{server}}_port, fixed_ips, 0, ip_address] }
            {%- endfor %}
            $server_addresses:
              list_join:
                - ','
                - - { get_attr: [server1_port, fixed_ips, 0, ip_address] }
                  {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
                  - { get_attr: [server{{server}}_port, fixed_ips, 0, ip_address] }
                  {%- endfor %}
          template: |
            #!/bin/sh
            echo "hadoop{{server}} install start"

            cat <<EOF>> /etc/ssh/ssh_config
            Port 20022
            EOF

            sudo service sshd restart


            cat <<EOF> /home/centos/.ssh/kepri-msa_default.pem
            $private_key
            EOF

            chown centos.centos /home/centos/.ssh/kepri-msa_default.pem
            chmod 600 /home/centos/.ssh/kepri-msa_default.pem

            echo "[01] host setting"
            echo "##### 01.1 host file setting ###########"
            cat <<EOF>> /etc/hosts
            {% if octaviaLbUse %}
            $vip_address hmaster
            {% elif %}
            $master_address hmaster
            {% endif %}
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            $node{{worker}}_address hworker-{{worker}}
            {%- endfor %}
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            mkdir -p /home/centos/hadoop/pids
            mkdir -p /home/centos/hadoop/etc/hadoop/dfs/data
            mkdir -p /home/centos/hadoop/etc/hadoop/dfs/name
            mkdir -p /home/centos/hadoop/etc/hadoop/dfs/journalnode
            sudo chmod 755 /home/centos/hadoop/pids
            sudo chmod 755 /home/centos/hadoop/etc/hadoop/dfs/data
            sudo chmod 755 /home/centos/hadoop/etc/hadoop/dfs/name
            sudo chmod 755 /home/centos/hadoop/etc/hadoop/dfs/journalnode
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            echo "##### 01.2 add_known_hosts ###########"
            ssh-keyscan -p 20022 hmaster,$master_addres >> /home/centos/.ssh/known_hosts
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            ssh-keyscan -p 20022 hworker-{{worker}},$node{{worker}}_address >> /home/centos/.ssh/known_hosts
            {%- endfor %}
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            echo "##### 01.3 add_authorized_keys ###########"
            ssh-keygen -b 2048 -t rsa -f /home/centos/.ssh/id_rsa -q -N ''
            cat /home/centos/.ssh/id_rsa.pub | ssh -i "/home/centos/.ssh/kepri-msa_default.pem" centos@hmaster  "cat - >> /home/centos/.ssh/authorized_keys"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            cat /home/centos/.ssh/id_rsa.pub | ssh -i "/home/centos/.ssh/kepri-msa_default.pem" centos@hworker-{{worker}}  "cat - >> /home/centos/.ssh/authorized_keys"
            {%- endfor %}
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            cp ~/hadoop/share/hadoop/tools/lib/hadoop-aws-2.7.7.jar ~/hadoop/share/hadoop/common/lib/
            cp ~/hadoop/share/hadoop/tools/lib/aws-java-sdk-1.7.4.jar ~/hadoop/share/hadoop/common/lib/
            cp ~/hadoop/share/hadoop/tools/lib/joda-time-2.10.jar ~/hadoop/share/hadoop/common/lib/
            cp ~/hadoop/share/hadoop/tools/lib/jackson-*.jar  ~/hadoop/share/hadoop/common/lib/
            export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/hadoop_env.sh
            export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*
            EOF

            echo "[01]hadoop config file update"
            cat <<EOF>> /home/centos/hadoop/etc/hadoop/core-site.xml
            <configuration>
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://hmaster:9000</value>
            </property>
            <property>
                 <name>ha.zookeeper.quorum</name>
                 <value>hmaster:2181{%- for worker in range(1,pworker) %}{% set server = worker + 1 %},hworker-{{worker}}:2181{%- endfor %}</value>
            </property>
            <property>
                <name>fs.s3a.impl</name>
                <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
            </property>
            <property>
              <name>fs.s3a.aws.credentials.provider</name>
              <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
            </property>
            <property>
              <name>fs.s3a.access.key</name>
              <value><![CDATA[$s3_accessKey]]></value>
            </property>
            <property>
              <name>fs.s3a.secret.key</name>
              <value><![CDATA[$s3_secretKey]]></value>
            </property>
            <property>
              <name>fs.s3a.connection.ssl.enabled</name>
              <value>false</value>
            </property>
            <property>
              <name>fs.s3a.endpoint</name>
              <value><![CDATA[$s3_endpoint]]></value>
            </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/hdfs-site.xml
            <configuration>
                    <property>
                            <name>dfs.namenode.name.dir</name>
                            <value>/home/centos/hadoop/etc/hadoop/dfs/name</value>
                    </property>
                    <property>
                            <name>dfs.datanode.data.dir</name>
                            <value>/home/centos/hadoop/etc/hadoop/dfs/data</value>
                    </property>
                    <property>
                            <name>dfs.journalnode.edits.dir</name>
                            <value>/home/centos/hadoop/etc/hadoop/dfs/journal</value>
                    </property>
                    <property>
                            <name>dfs.nameservices</name>
                            <value>hadoop-cluster</value>
                    </property>
                    <property>
                            <name>dfs.ha.namenodes.hadoop-cluster</name>
                            <value>nn1,nn2</value>
                    </property>
                    <property>
                            <name>dfs.namenode.rpc-address.hadoop-cluster.nn1</name>
                            <value>hmaster:9000</value>
                    </property>
                    <property>
                            <name>dfs.namenode.rpc-address.hadoop-cluster.nn2</name>
                            <value>hworker-1:9000</value>
                    </property>
                    <property>
                            <name>dfs.namenode.http-address.hadoop-cluster.nn1</name>
                            <value>hmaster:50070</value>
                    </property>
                    <property>
                            <name>dfs.namenode.http-address.hadoop-cluster.nn2</name>
                            <value>hworker-1:50070</value>
                    </property>
                    <property>
                            <name>dfs.namenode.shared.edits.dir</name>
                            <value>qjournal://hmaster:8485{%- for worker in range(1,pworker) %}{% set server = worker + 1 %};hworker-{{worker}}:8485{%- endfor %}/hadoop-cluster</value>
                    </property>
                    <property>
                            <name>dfs.client.failover.proxy.provider.hadoop-cluster</name>
                            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
                    </property>
                    <property>
                            <name>dfs.ha.fencing.methods</name>
                            <value>sshfence</value>
                    </property>
                    <property>
                            <name>dfs.ha.fencing.ssh.private-key-files</name>
                            <value>/home/centos/.ssh/id_rsa</value>
                    </property>
                    <property>
                            <name>dfs.ha.automatic-failover.enabled</name>
                            <value>true</value>
                    </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/mapred-site.xml
            <configuration>
                <property>
                    <name>mapreduce.job.ubertask.enable</name>
                    <value>true</value>
                </property>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/yarn-site.xml
            <configuration>
            <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
            </property>
            <property>
                <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
            </property>
            <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hmaster</value>
            </property>
            <property>
                <name>yarn.application.classpath</name>
                <value>
                    /home/centos/hadoop/etc/hadoop,
                    /home/centos/hadoop/share/hadoop/common/*,
                    /home/centos/hadoop/share/hadoop/common/lib/*,
                    /home/centos/hadoop/share/hadoop/hdfs/*,
                    /home/centos/hadoop/share/hadoop/hdfs/lib/*,
                    /home/centos/hadoop/share/hadoop/mapreduce/*,
                    /home/centos/hadoop/share/hadoop/mapreduce/lib/*,
                    /home/centos/hadoop/share/hadoop/yarn/*,
                    /home/centos/hadoop/share/hadoop/yarn/lib/*
                </value>
            </property>
            </configuration>
            EOF

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            echo "##### 03.zookeeper setting start ###########"
            mkdir /home/centos/zookeeper/tmp
            cp /home/centos/zookeeper/conf/zoo_sample.cfg /home/centos/zookeeper/conf/zoo.cfg

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            sudo -- sh -c "echo dataDir=/home/centos/zookeeper/tmp >> /home/centos/zookeeper/conf/zoo.cfg"
            sudo -- sh -c "echo server.1=hmaster:2888:3888 >> /home/centos/zookeeper/conf/zoo.cfg"
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo -- sh -c "echo server.{{server}}=hworker-{{worker}}:2888:3888 >> /home/centos/zookeeper/conf/zoo.cfg"
            {%- endfor %}
            EOF

            echo "{{server}}" > /home/centos/zookeeper/tmp/myid
            chown centos:centos /home/centos/zookeeper/tmp/myid
            chmod 777 /home/centos/zookeeper/tmp/myid
            EOF
            sudo -i -u centos bash << EOF
            source ~/.bashrc
            /home/centos/zookeeper/bin/zkServer.sh start
            echo "##### 03.zookeeper setting end ###########"
            EOF

{% if hbaseUse %}
            echo "##### 04.hbase setting start ###########"
            VAR1=$(cat <<EOF
            <?xml version="1.0"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>hbase.rootdir</name>
                    <value>hdfs://hmaster:9000/hbase</value>
                </property>
                <property>
                    <name>hbase.cluster.distributed</name>
                    <value>true</value>
                </property>
                <property>
                    <name>hbase.zookeeper.quorum</name>
                    <value>hmaster{%- for worker in range(1,pworker) %}{% set server = worker + 1 %},hworker-{{worker}}{%- endfor %}</value>
                </property>
                <property>
                    <name>dfs.replication</name>
                    <value>3</value>
                </property>
                <property>
                  <name>hbase.unsafe.stream.capability.enforce</name>
                  <value>false</value>
                </property>
                <property>
                    <name>hbase.zookeeper.property.clientPort</name>
                    <value>2181</value>
                </property>
            </configuration>
            EOF
            )

            cat <<EOF> /home/centos/hbase/conf/hbase-site.xml
            ${VAR1}
            EOF

            chown centos:centos /home/centos/hbase/conf/hbase-site.xml

            cat <<EOF> /home/centos/hbase/conf/regionservers
            hmaster
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            hworker-{{worker}}
            {%- endfor %}
            EOF

            chown centos:centos /home/centos/hbase/conf/regionservers

            cat <<EOF>> /home/centos/hbase/conf/hbase-env.sh
            export HBASE_MANAGES_ZK=false
            export HBASE_PID_DIR=/home/centos/hbase/pid
            EOF

            echo "##### 04.hbase setting end ###########"
{% endif %}

{% if sparkUse %}
            echo "##### 02.spark setting start ###########"

            masterRun=1
            while [[ masterRun -eq 1 ]]
            do
              echo "nc -z $vip_address 7077"
              nc -z $vip_address 7077
              masterRun=$?
              sleep 5
            done
            echo "master node active"

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            sudo -- sh -c "echo hworker-{{worker}} >> /home/centos/spark/conf/slaves"
            {%- endfor %}
            /home/centos/spark/sbin/start-slave.sh 'spark://hmaster:7077'
            EOF

            echo "##### 02.spark setting end ###########"
  {% endif %}

  {%- endfor %}

  server1:
    type: OS::Nova::Server
    properties:
      flavor: { get_param: master_flavor }
#      image: { get_param: image }
      block_device_mapping_v2:
         - image: { get_param: image }
           delete_on_termination: true
           volume_size: { get_param: master_flavor_volume_size }
           boot_index: 0
      availability_zone: { get_param: availability_zone }
      networks:
        - port: { get_resource: server1_port }
      key_name: { get_param: key_name}
      user_data_format: RAW
      user_data:
        get_resource: init_config1

  {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
  server{{server}}:
    type: OS::Nova::Server
    properties:
      flavor: { get_param: worker_flavor }
#      image: { get_param: image }
      block_device_mapping_v2:
         - image: { get_param: image }
           delete_on_termination: true
           volume_size: { get_param: worker_flavor_volume_size }
           boot_index: 0
      availability_zone: { get_param: availability_zone }
      networks:
        - port: { get_resource: server{{server}}_port }
      key_name: { get_param: key_name}
      user_data_format: RAW
      user_data:
        get_resource: init_config{{server}}
  {%- endfor %}

{% if octaviaLbUse %}
  octavia_lb:
    type: OS::Octavia::LoadBalancer
    properties:
      vip_subnet: { get_param: provider_subnet }
      description: { get_param: lb_description }

  octavia_lb_svc_listener:
    type: OS::Octavia::Listener
    properties:
      loadbalancer: { get_resource: octavia_lb }
      connection_limit: { get_param: lb_svc_connection_limit }
      protocol: { get_param: lb_svc_protocol }
      protocol_port: { get_param: lb_svc_port }

  octavia_lb_svc_pool:
    type: OS::Octavia::Pool
    properties:
      listener: { get_resource: octavia_lb_svc_listener }
      lb_algorithm: { get_param: lb_algorithm }
      protocol: { get_param: lb_svc_protocol }

  octavia_lb_svc_monitor:
    type: OS::Octavia::HealthMonitor
    properties:
      pool: { get_resource: octavia_lb_svc_pool }
      type: { get_param: lb_svc_monitor_type }
      delay: { get_param: lb_svc_monitor_delay }
      max_retries: { get_param: lb_svc_monitor_max_retries }
      timeout: { get_param: lb_svc_monitor_timeout }
{% if monitorUrlPathUse %}
      url_path: { get_param: lb_svc_monitor_url_path }
{% endif %}
  {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
  lb_svc_pool_member{{worker}}:
    type: OS::Octavia::PoolMember
    properties:
      pool: { get_resource: octavia_lb_svc_pool }
      address: { get_attr: [server{{worker}}_port, fixed_ips, 0, ip_address] }
      protocol_port: { get_param: service_port }
  {%- endfor %}
{% endif %}

outputs:
  servers:
    value:
      - instance: { get_attr: [ server1, show ] }
        instancePort: { get_attr: [ server1_port, show ] }
      {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
      - instance: { get_attr: [ server{{server}}, show ] }
        instancePort: { get_attr: [ server{{server}}_port, show ] }
      {%- endfor %}
{% if octaviaLbUse %}
  octaviaLb:
    value:
      loadBalancer: { get_attr: [ octavia_lb, show ] }
      listeners:
        - listener: { get_attr: [ octavia_lb_svc_listener, show ] }
          pool: { get_attr: [ octavia_lb_svc_pool, show ] }
          monitor: { get_attr: [ octavia_lb_svc_monitor, show ] }
          poolMembers:
            {%- for worker in range(1,pworker) %} {% set server = worker + 1 %}
            - { get_attr: [ lb_svc_pool_member{{worker}}, show ] }
            {%- endfor %}
{% endif %}
