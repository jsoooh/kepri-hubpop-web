heat_template_version: '2018-08-31'

description: hadoop stand alone template

parameters:
  api_end_point:
    type: string
  image:
    type: string
    default: hadoop-base2-3
  availability_zone:
    type: string
  flavor:
    type: string
  key_name:
    type: string
  security_group:
    type: string
  provider_net:
    type: string
  provider_subnet:
    type: string
  root_password:
    label: "root 비밀번호"
    type: string
    default: crossent12
  hive_password:
    label: "hive 비밀번호"
    type: string
    default: hive1234

resources:

  server1_port:
    type: OS::Neutron::Port
    properties:
      fixed_ips: [{ subnet_id: { get_param: provider_subnet } }]
      network: { get_param: provider_net }
      security_groups: [{ get_param: security_group }]

  init_config1:
    type: OS::Heat::SoftwareConfig
    properties:
      group: ungrouped
      config:
        str_replace:
          params:
            $api_end_point: { get_param: api_end_point }
            $project_id: { get_param: 'OS::project_id' }
            $stack_id: { get_param: 'OS::stack_id' }
            $stack_name: { get_param: 'OS::stack_name' }
            $root_password: { get_param: root_password }
            $hive_password: { get_param: hive_password }
            $master_address: { get_attr: [server1_port, fixed_ips, 0, ip_address] }
            $server_addresses:
              list_join:
                - ','
                - - { get_attr: [server1_port, fixed_ips, 0, ip_address] }
          template: |
            #!/bin/sh
            echo "01.hadoop core install start ......"
            curl -X PUT -H "Content-Type: application/json" --connect-timeout 10 --data "{\"deployStatus\": \"DEPLOY_IN_PROGRESS\"}" $api_end_point/vm_catalog/external/$project_id/deploy/$stack_name/$stack_id/status

            sudo -i -u centos bash << EOF
            source ~/.bashrc
            echo "[00]key setup"
            ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
            cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
            chmod 0600 ~/.ssh/authorized_keys
            ssh-keyscan -H localhost,0.0.0.0 >> /home/centos/.ssh/known_hosts
            EOF

            echo "[01]hadoop config file update"
            cat <<EOF>> /home/centos/hadoop/etc/hadoop/core-site.xml
            <configuration>
                    <property>
                        <name>fs.default.name</name>
                        <value>hdfs://localhost:9000</value>
                    </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/hdfs-site.xml
            <configuration>
                <property>
                        <name>dfs.replication</name>
                        <value>1</value>
                </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/mapred-site.xml
            <configuration>
                <property>
                    <name>mapreduce.framework.name</name>
                    <value>yarn</value>
                </property>
            </configuration>
            EOF

            cat <<EOF>> /home/centos/hadoop/etc/hadoop/yarn-site.xml
            <configuration>
                <property>
                    <name>yarn.nodemanager.aux-services</name>
                    <value>mapreduce_shuffle</value>
                </property>
            </configuration>
            EOF

            echo "#####[02]hdfs namenode -format#####"
            sudo su - centos -c "hadoop namenode -format -force "
            echo "##### [02].hdfs namenode -format ###########"

            echo "#####[03] start-dfs #####"
            sudo su - centos -c "start-dfs.sh "
            echo "##### [03].start-dfs end ###########"

            echo "#####[04]start-yarn #####"
            sudo su - centos -c "start-yarn.sh "
            echo "##### [04].start-dfs end ###########"

            echo "##### [05]hive setting start ###########"
            echo "mariadb setting start"
            #cp /etc/my.cnf /etc/my.cnf.backup
            #sed -i 's/^port            = 3306/port            = $service_port/' /etc/my.cnf
            sleep 1
            systemctl start mariadb
            /usr/bin/mysqladmin -u root password '$root_password'

            mysql -u root -p$root_password <<EOF
            create database hive CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
            GRANT ALL PRIVILEGES ON hive.* To 'hive'@LOCALHOST IDENTIFIED BY '$hive_password';
            EOF

            sudo systemctl enable mariadb
            echo "mariadb setting end"

            cat <<EOF> /home/centos/hive/conf/hive-site.xml
            <?xml version="1.0" encoding="UTF-8" standalone="no"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>hive.metastore.local</name>
                    <value>false</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionURL</name>
                    <value>jdbc:mariadb://localhost:3306/hive?createDatabaseIfNotExist=true</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionDriverName</name>
                    <value>org.mariadb.jdbc.Driver</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionUserName</name>
                    <value>hive</value>
                </property>
                <property>
                    <name>javax.jdo.option.ConnectionPassword</name>
                    <value>hive1234</value>
                </property>
            </configuration>
            EOF

            chown centos:centos /home/centos/hive/conf/hive-site.xml
            echo "## MariaDB connetc init "
            sudo su - centos -c "/home/centos/hive/bin/schematool -initSchema -dbType mysql "
            echo "##### [05]hive setting end ###########"

{% if sparkUse %}
            echo "##### [06]spark setting start ###########"
            cat <<EOF>> /home/centos/spark/conf/spark-env.sh
            export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64
            export HADOOP_CONF_DIR=~/hadoop/etc/hadoop
            export SPARK_MASTER_IP=localhost
            export SPARK_MASTER_PORT=7077
            export SPARK_WORKER_INSTANCES=1
            export SPARK_WORKER_MEMORY=8192m
            export SPARK_WORKER_CORES=8
            export SPARK_MASTER_OPTS="-Dspark.deploy.defaultCores=5"
            EOF

            #sudo su - centos -c "echo localhost >> /home/centos/spark/conf/slaves"
            #sudo su - centos -c "/home/centos/spark/sbin/start-slave.sh 'spark://hmaster:7077' "
            echo "## spark start-all ## "
            sudo su - centos -c "/home/centos/spark/sbin/start-all.sh  "
            echo "##### [06]spark setting end ###########"
{% endif %}

{% if hbaseUse %}
            echo "##### [07].hbase setting start ###########"
            cat <<EOF> /home/centos/hbase/conf/hbase-site.xml
            <?xml version="1.0"?>
            <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
            <configuration>
                <property>
                    <name>hbase.rootdir</name>
                    <value>hdfs://localhost:9000/hbase</value>
                </property>
            </configuration>
            EOF

            chown centos:centos /home/centos/hbase/conf/hbase-site.xml

            cat <<EOF>> /home/centos/hbase/conf/hbase-env.sh
            export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64
            export HBASE_MANAGES_ZK=false
            #export HBASE_PID_DIR=/home/centos/hbase/pid
            EOF

            echo "## hbase start-hbase ## "
            sudo su - centos -c "/home/centos/hbase/bin/start-hbase.sh "

            echo "##### 04.hbase setting end ###########"
{% endif %}

            curl -X PUT -H "Content-Type: application/json" --connect-timeout 10 --data "{\"deployStatus\": \"DEPLOY_COMPLETE\"}" $api_end_point/vm_catalog/external/$project_id/deploy/$stack_name/$stack_id/status
            echo "01.hadoop core install end ......"

  server1:
    type: OS::Nova::Server
    properties:
      flavor: { get_param: flavor }
      image: { get_param: image }
      availability_zone: { get_param: availability_zone }
      networks:
        - port: { get_resource: server1_port }
      key_name: { get_param: key_name}
      user_data_update_policy: IGNORE
      user_data_format: RAW
      user_data:
        get_resource: init_config1